{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/matplotlib/__init__.py:1357: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import matplotlib\n",
    "import math\n",
    "matplotlib.use(\"svg\")\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3006, 34)\n",
      "(3006,)\n",
      "(6012, 34)\n",
      "(6012,)\n"
     ]
    }
   ],
   "source": [
    "# init\n",
    "filename=\"features/blues/blues.00000.csv\"\n",
    "# filename=\"feature_txt/blues/blues.00000.txt\"\n",
    "X = np.loadtxt(filename,delimiter=',')\n",
    "print X.shape\n",
    "y=np.ones(X.shape[0])\n",
    "print y.shape\n",
    "\n",
    "# filenames=[\"classical\",\"country\",\"disco\",\"hiphop\",\"jazz\",\"metal\",\"pop\",\"reggae\",\"rock\"]\n",
    "filenames=[\"classical\"]\n",
    "for filename in filenames:\n",
    "#     X0=np.loadtxt(\"feature_txt/\"+filename+\"/\"+filename+\".00000.txt\",delimiter=',')\n",
    "    X0=np.loadtxt(\"features/\"+filename+\"/\"+filename+\".00000.csv\",delimiter=',')\n",
    "    X=np.vstack((X,X0))\n",
    "    y=np.concatenate((y,np.zeros(X0.shape[0])))\n",
    "print X.shape\n",
    "print y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def __init__(self, X, y, reg):\n",
    "        \"\"\" Initialize the SVM attributes and initialize the weights vector to the zero vector. \n",
    "            Attributes: \n",
    "                X (array_like) : training data intputs\n",
    "                y (vector) : 1D numpy array of training data outputs\n",
    "                reg (float) : regularizer parameter\n",
    "                theta : 1D numpy array of weights\n",
    "        \"\"\"\n",
    "        self.X = sp.csr_matrix(X)\n",
    "        self.y = sp.csr_matrix(y)\n",
    "        self.reg = reg\n",
    "        self.theta = sp.csr_matrix(np.zeros(X.shape[1]))\n",
    "        \n",
    "    \n",
    "    def objective(self, X, y):\n",
    "        \"\"\" Calculate the objective value of the SVM. When given the training data (self.X, self.y), this is the \n",
    "            actual objective being optimized. \n",
    "            Args:\n",
    "                X (array_like) : array of examples, where each row is an example\n",
    "                y (array_like) : array of outputs for the training examples\n",
    "            Output:\n",
    "                (float) : objective value of the SVM when calculated on X,y\n",
    "        \"\"\"\n",
    "        y_coo=sp.csr_matrix(y)\n",
    "        X_coo=sp.csr_matrix(X)\n",
    "        y_my_coo=X_coo.dot(self.theta.T)\n",
    "        ones=sp.csr_matrix(np.ones(y.shape[1]))\n",
    "        tmp=sp.csr_matrix(ones-y_coo.multiply(y_my_coo.T))\n",
    "        positive=(tmp>0)+0\n",
    "        return (tmp.multiply(positive)).sum()+0.5*self.reg*np.linalg.norm(self.theta)**2\n",
    "        \n",
    "        \n",
    "    def gradient(self):\n",
    "        \"\"\" Calculate the gradient of the objective value on the training examples. \n",
    "            Output:\n",
    "                (vector) : 1D numpy array containing the gradient\n",
    "        \"\"\"\n",
    "        y_my_coo=(self.X).dot(self.theta.T)\n",
    "        product_y=(self.y).multiply(y_my_coo.T)\n",
    "        ones=sp.csr_matrix(np.ones(product_y.shape))\n",
    "        mask=ones-((product_y>1)+0)  # 0/1\n",
    "        coe_X_coo=(self.y).multiply(mask)*(-1)\n",
    "        return sp.csr_matrix(coe_X_coo.dot(self.X)+self.reg*self.theta).toarray()[0]\n",
    "\n",
    "        \n",
    "    def train(self, niters=100, learning_rate=1, verbose=False):\n",
    "        \"\"\" Train the support vector machine with the given parameters. \n",
    "            Args: \n",
    "                niters (int) : the number of iterations of gradient descent to run\n",
    "                learning_rate (float) : the learning rate (or step size) to use when training\n",
    "                verbose (bool) : an optional parameter that you can use to print useful information (like objective value)\n",
    "        \"\"\"\n",
    "        for i in range(niters):\n",
    "            gradient=self.gradient()\n",
    "            self.theta=self.theta-learning_rate*gradient\n",
    "            if verbose:\n",
    "                print 'obj=',self.objective(self.X,self.y)\n",
    "            \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict the class of each label in X. \n",
    "            Args: \n",
    "                X (array_like) : array of examples, where each row is an example\n",
    "            Output:\n",
    "                (vector) : 1D numpy array containing predicted labels\n",
    "        \"\"\"\n",
    "        X_coo=sp.csr_matrix(X)\n",
    "        y_my_coo=X_coo.dot(self.theta.T).T\n",
    "        return sp.csr_matrix(((y_my_coo>=0)+0)*2-1).toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.509966777409\n"
     ]
    }
   ],
   "source": [
    "m=X.shape[0]\n",
    "split=int(math.floor(m*0.9))\n",
    "P=np.random.permutation(m)\n",
    "X_tr=X[P[0:split]]\n",
    "X_te=X[P[split:m]]\n",
    "y_tr=y[P[0:split]]\n",
    "y_te=y[P[split:m]]\n",
    "reg=1e-4\n",
    "svm=SVM(X_tr, y_tr, reg)\n",
    "svm.train()\n",
    "y_p=svm.predict(X_te)\n",
    "accuracy=(y_p==y_te).sum()/float(len(y_te))\n",
    "print 'accuracy =',accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30075, 34)\n",
      "(30075,)\n"
     ]
    }
   ],
   "source": [
    "# init\n",
    "# n=13\n",
    "n=34\n",
    "X=np.ones((0,n))\n",
    "y=np.asarray([])\n",
    "filenames=[\"blues\",\"classical\",\"country\",\"disco\",\"hiphop\",\"jazz\",\"metal\",\"pop\",\"reggae\",\"rock\"]\n",
    "for i,filename in enumerate(filenames):\n",
    "#     X0=np.loadtxt(\"feature_txt/\"+filename+\"/\"+filename+\".00000.txt\",delimiter=',')\n",
    "    X0=np.loadtxt(\"features/\"+filename+\"/\"+filename+\".00000.csv\",delimiter=',')\n",
    "    X=np.vstack((X,X0))\n",
    "    y=np.concatenate((y,np.repeat(i,X0.shape[0])))\n",
    "print X.shape\n",
    "print y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_loss(X, Theta, y, return_grad=False):\n",
    "    \"\"\"\n",
    "    Compute softmax loss and its gradient.\n",
    "\n",
    "    Args:\n",
    "        X : 2D numpy array, size m x n: array of all input features for all examples\n",
    "        Theta: 2D numpy array, size n x p: parameter matrix\n",
    "        y: 1D numpy array, size m: containing indices of correct outputs (zero-indexed)\n",
    "        return_grad: boolean, whether or not to return gradients\n",
    "    \n",
    "    Returns: loss, or (loss, gradient) if return_grad = True\n",
    "        loss: the average softmax loss over all examples\n",
    "        gradient: gradient w.r.t. theta of average loss \n",
    "    \"\"\"\n",
    "    Y=np.array(y[:,None]==np.arange(Theta.shape[1])[None,:],dtype=np.float64)\n",
    "    Yp=X.dot(Theta)\n",
    "#     exp_Yp=np.exp(Yp)\n",
    "    exp_Yp=np.exp(Yp-Yp.max(axis=1)[:,None]) # overflow\n",
    "    sum_exp_Yp=np.sum(exp_Yp,axis=1)\n",
    "    loss=(np.sum(np.log(sum_exp_Yp))-np.sum(Yp*Y))/X.shape[0] \n",
    "    if return_grad:\n",
    "        G=X.T.dot(exp_Yp/sum_exp_Yp[:,None]-Y)/X.shape[0]\n",
    "        return loss,G\n",
    "    else:\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time = 0.018214225769\n",
      "2.30258509299 <type 'numpy.ndarray'> (34, 10) 1.06245060377\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "m=X.shape[0]\n",
    "split=int(math.floor(m*0.9))\n",
    "P=np.random.permutation(m)\n",
    "X_tr=X[P[0:split]]\n",
    "X_te=X[P[split:m]]\n",
    "y_tr=y[P[0:split]]\n",
    "y_te=y[P[split:m]]\n",
    "start=time.time()\n",
    "loss, G = softmax_loss(X_tr,np.zeros((X_tr.shape[1], int(np.max(y_tr))+1)), y_tr, return_grad=True)\n",
    "end=time.time()\n",
    "print 'total time =',end-start\n",
    "print loss, type(G), G.shape, np.linalg.norm(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_gd(X, y, X_test, y_test, lam=1e-5, alpha=1.0, iters=100):\n",
    "    \"\"\"\n",
    "    Gradient descent to minimize softmax loss.\n",
    "    \n",
    "    Args:\n",
    "        X : 2D numpy array, size m x n, array of all input features for all trainingexamples\n",
    "        y: 1D numpy array, size m, contains indices of correct outputs on training set (zero-indexed)\n",
    "        X_test : 2D numpy array, size m0 x n, array of all input features for all testing examples\n",
    "        y_test: 1D numpy array, size m0, contains indices of correct outputs on testing set (zero-indexed)\n",
    "        lam: regularization parameter\n",
    "        alpha: step size\n",
    "        iters: number of iterations of gradient descent\n",
    "    \n",
    "    Returns: Theta\n",
    "        Theta: matrix of parameters for softmax model\n",
    "    \"\"\"\n",
    "    Theta=np.zeros((X.shape[1],int(np.max(y))+1))\n",
    "    for i in range(iters):\n",
    "        loss,G=softmax_loss(X,Theta,y,return_grad=True)\n",
    "        Theta-=alpha*(G+lam*Theta)\n",
    "    return Theta      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'> (34, 10) 48.3300809037\n",
      "time = 1.54382610321\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "Theta = softmax_gd(X_tr, y_tr, X_te, y_te,iters=100)\n",
    "print type(Theta), Theta.shape, np.linalg.norm(Theta)\n",
    "end=time.time()\n",
    "print 'time =',end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.193151595745\n"
     ]
    }
   ],
   "source": [
    "y_p=X_te.dot(Theta).argmax(axis=1)\n",
    "accuracy=(y_p==y_te).sum()/float(len(y_te))\n",
    "print 'accuracy =',accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
